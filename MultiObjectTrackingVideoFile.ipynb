{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f07a3d",
   "metadata": {},
   "source": [
    "## Multi Object Tracking sample\n",
    "This notebook is an example how to perform object detection with multi-object tracking (MOT) from a video file to count vehicle traffic.\n",
    "The **ByteTracker** is used for multi-object tracking (see https://github.com/ifzhang/ByteTrack)\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863f7c-973f-4df8-84a2-7a9ef9f4a7bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify model name and input file parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7290d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model name to be used for inference\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca_1\"\n",
    "# input video file\n",
    "input_filename = 'images/Traffic.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c7692-360b-49fc-bf3b-afe5c6992b8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c2089-7d54-4a7d-8e4d-c490f4b67b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "cloud_token = mytools.get_token() # get cloud API access token from env.ini file\n",
    "cloud_zoo_url = mytools.get_cloud_zoo_url() # get cloud zoo URL from env.ini file\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "# 1. Inference on the DeGirum Cloud Platform\n",
    "zoo = dg.connect(dg.CLOUD, cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 2. Inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN\n",
    "# zoo = dg.connect(mytools.get_ai_server_hostname(), cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 3. Inference on DeGirum ORCA accelerator installed on your computer\n",
    "# zoo = dg.connect(dg.LOCAL, cloud_zoo_url, cloud_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d41d1-85f6-442c-a3d3-4a7fafaebcef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6121d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import IPython.display\n",
    "lap = mytools.import_optional_package(\"lap\")\n",
    "cython_bbox = mytools.import_optional_package(\"cython_bbox\")\n",
    "from mot.byte_tracker import BYTETracker\n",
    "from mot.basetrack import BaseTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0ef34-f06d-494f-8883-b9c87aae5693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load object detection model\n",
    "model = zoo.load_model(model_name)\n",
    "\n",
    "# set model parameters\n",
    "model.image_backend = 'opencv' # select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "model.input_numpy_colorspace = 'BGR'\n",
    "model.overlay_show_probabilities = True\n",
    "model.overlay_line_width = 1\n",
    "model._model_parameters.InputImgFmt = ['JPEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a07f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# video input and output\n",
    "orig_path = Path(input_filename)\n",
    "ann_path = orig_path.with_name(orig_path.stem + \"_annotated\" + orig_path.suffix) # this is output path, you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8155b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dict_dot_notation(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "# return bool, check line intersect\n",
    "def intersect(a, b, c, d):\n",
    "    s = (a[0] - b[0]) * (c[1] - a[1]) - (a[1] - b[1]) * (c[0] - a[0])\n",
    "    t = (a[0] - b[0]) * (d[1] - a[1]) - (a[1] - b[1]) * (d[0] - a[0])\n",
    "    if s * t > 0:\n",
    "        return False\n",
    "    s = (c[0] - d[0]) * (a[1] - c[1]) - (c[1] - d[1]) * (a[0] - c[0])\n",
    "    t = (c[0] - d[0]) * (b[1] - c[1]) - (c[1] - d[1]) * (b[0] - c[0])\n",
    "    if s * t > 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460a077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AI prediction loop\n",
    "# this loop make a video to image folder with suffix \"_annotated\"\n",
    "with mytools.open_video_stream(input_filename) as stream:\n",
    "    \n",
    "    image_w = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    image_h = int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # count line (x, y)\n",
    "    line_start = (0, 2 * image_h // 3)\n",
    "    line_end = (image_w, line_start[1])\n",
    "\n",
    "    # counters for each direction\n",
    "    left = right = top = bottom = 0\n",
    "    \n",
    "    BaseTrack._count = 0 # reset track counter\n",
    "    \n",
    "    with mytools.Display(\"MoT\") as display, \\\n",
    "         mytools.open_video_writer(str(ann_path), image_w, image_h) as writer:\n",
    "    \n",
    "        fps = 30 # you can specify input video FPS if you want\n",
    "        tracker = BYTETracker(\n",
    "            args=dict_dot_notation({\n",
    "                'track_thresh': 0.3,\n",
    "                'track_buffer': fps * 2,\n",
    "                'match_thresh': 0.8,\n",
    "                'mot20': False,\n",
    "            }),\n",
    "            frame_rate=fps\n",
    "        )\n",
    "        timeout_count_dict = {}\n",
    "        is_counted_dict = {}\n",
    "        trail_dict = {}\n",
    "        timeout_count_initial = fps\n",
    "\n",
    "        progress = mytools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        for batch_result in model.predict_batch(mytools.video_source(stream, report_error=False)):\n",
    "            # object detection\n",
    "            results = batch_result.results\n",
    "            bboxes = np.zeros((len(results), 5))\n",
    "            image = batch_result.image\n",
    "\n",
    "            # byte track\n",
    "            for index, result in enumerate(results):\n",
    "                bbox = np.array(result.get('bbox', [0, 0, 0, 0]))\n",
    "                score = result.get('score', 0)\n",
    "                bbox_and_score = np.append(bbox, score)\n",
    "                bboxes[index] = bbox_and_score\n",
    "\n",
    "            online_targets = tracker.update(bboxes, (1, 1), (1, 1))\n",
    "            online_target_set = set([])\n",
    "\n",
    "            # tracking start or continue\n",
    "            for target in online_targets:\n",
    "                tid = str(target.track_id)\n",
    "                online_target_set.add(str(tid))\n",
    "\n",
    "                box = tuple(map(int, target.tlbr)) # x1 y1 x2 y2\n",
    "                center = tuple(map(int, target.tlwh_to_xyah(target.tlwh)[:2]))\n",
    "                if trail_dict.get(tid, None) is None:\n",
    "                    trail_dict[tid] = []\n",
    "                if is_counted_dict.get(tid, None) is None:\n",
    "                    is_counted_dict[tid] = False\n",
    "                if not is_counted_dict[tid] and len(trail_dict[tid]) > 1:\n",
    "                    trail_start = trail_dict[tid][0]\n",
    "                    trail_end = center\n",
    "                    is_cross = intersect(line_start, line_end, trail_start, trail_end)\n",
    "                    if is_cross:\n",
    "                        if trail_start[0] > trail_end[0]:\n",
    "                            left += 1\n",
    "                        if trail_start[0] < trail_end[0]:\n",
    "                            right += 1\n",
    "                        if trail_start[1] < trail_end[1]:\n",
    "                            top += 1\n",
    "                        if trail_start[1] > trail_end[1]:\n",
    "                            bottom += 1\n",
    "                        is_counted_dict[tid] = True\n",
    "                trail_dict[tid].append(center)\n",
    "                timeout_count_dict[tid] = timeout_count_initial\n",
    "                if len(trail_dict[tid]) > 1:\n",
    "                    cv2.polylines(image, [np.array(trail_dict[tid])], False, (255, 255, 0))\n",
    "                mytools.Display.put_text(image, tid, (box[0], box[3]), (255,255,255), (0,0,0), cv2.FONT_HERSHEY_PLAIN)\n",
    "                cv2.rectangle(image, box[0:2], box[2:4], color=(0, 255, 0), thickness=1)\n",
    "                cv2.drawMarker(image, center, (255, 255, 0), markerType=cv2.MARKER_CROSS)\n",
    "                \n",
    "\n",
    "            # tracking terminate\n",
    "            for tid in set(timeout_count_dict.keys()) - online_target_set:\n",
    "                timeout_count_dict[tid] -= 1\n",
    "                if timeout_count_dict[tid] == 0:\n",
    "                    del timeout_count_dict[tid], is_counted_dict[tid], trail_dict[tid]\n",
    "\n",
    "            text = 'Top={} Bottom={} Left={} Right={}'.format(top, bottom, left, right)\n",
    "            mytools.Display.put_text(image, text, (image_w // 3, 0), (255,255,255), (0,0,0), cv2.FONT_HERSHEY_PLAIN)\n",
    "            cv2.line(image, line_start, line_end, (0, 255, 0))\n",
    "\n",
    "            writer.write(image)\n",
    "            display.show(image)\n",
    "            progress.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211246e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display result\n",
    "IPython.display.Video(filename=str(ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original video\n",
    "IPython.display.Video(filename=str(orig_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "50de14cfcf8437409e83adf65890e3e47263b30fd21ab1f0117168323be0df4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

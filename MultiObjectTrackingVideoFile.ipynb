{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f07a3d",
   "metadata": {},
   "source": [
    "## Multi Object Tracking sample with DeGirum Cloud\n",
    "This demo measures vehicle traffic from video.\n",
    "Since I built it for non-real-time, I use cloud-inference, but if you want to use it for real-time, you can change to on-device inference as well.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. [DeGirum Cloud Platform](https://cs.degirum.com),\n",
    "1. DeGirum-hosted AI server node shared via Peer-to-Peer VPN,\n",
    "1. AI server node hosted by you in your local network,\n",
    "1. AI server running on your local machine,\n",
    "1. DeGirum ORCA accelerator directly installed on your local machine.\n",
    "\n",
    "To try different options, you just need to change the `inference_option` in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e6fc0",
   "metadata": {},
   "source": [
    "### If you fail to install 'cython_bbox', you should try to install it from github which url below.\n",
    "\n",
    "`pip install -e git+https://github.com/samson-wang/cython_bbox.git#egg=cython-bbox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6121d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import numpy as np\n",
    "import mytools, cv2\n",
    "from pathlib import Path\n",
    "\n",
    "from mot.byte_tracker import BYTETracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b94d7",
   "metadata": {},
   "source": [
    "I use ByteTracker for MoT.\n",
    "https://github.com/ifzhang/ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8155b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count line (x, y)\n",
    "line_start = (0, 360)\n",
    "line_end = (960, 360)\n",
    "\n",
    "# counter each directions\n",
    "left = 0\n",
    "right = 0\n",
    "top = 0\n",
    "bottom = 0\n",
    "\n",
    "class dict_dot_notation(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "# return bool, check line intersect\n",
    "def intersect(a, b, c, d):\n",
    "    s = (a[0] - b[0]) * (c[1] - a[1]) - (a[1] - b[1]) * (c[0] - a[0])\n",
    "    t = (a[0] - b[0]) * (d[1] - a[1]) - (a[1] - b[1]) * (d[0] - a[0])\n",
    "    if s * t > 0:\n",
    "        return False\n",
    "    s = (c[0] - d[0]) * (a[1] - c[1]) - (c[1] - d[1]) * (a[0] - c[0])\n",
    "    t = (c[0] - d[0]) * (b[1] - c[1]) - (c[1] - d[1]) * (b[0] - c[0])\n",
    "    if s * t > 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_option = 1 # <<< change it according to your needs selecting from the list in the header comment\n",
    "\n",
    "# connect to model zoo according to selected inference option\n",
    "zoo = mytools.connect_model_zoo(inference_option)\n",
    "\n",
    "# list model and choose your model (if you want to do traffic tracking, you should choose coco model)\n",
    "model_list = zoo.list_models()\n",
    "print(model_list)\n",
    "\n",
    "# load object detection model\n",
    "model = zoo.load_model(\"yolo_v5s_coco--512x512_quant_n2x_orca_1\")\n",
    "\n",
    "# set model parameters\n",
    "model.image_backend = 'opencv' # select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "model.input_numpy_colorspace = 'BGR'\n",
    "model.overlay_show_probabilities = True\n",
    "model.overlay_line_width = 1\n",
    "model._model_parameters.InputImgFmt = ['JPEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a07f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video input and output\n",
    "input_filename = 'images\\Traffic.mp4'\n",
    "orig_path = Path(input_filename)\n",
    "ann_path = orig_path.with_name(orig_path.stem + \"_annotated\" + orig_path.suffix) # this is output path, you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI prediction loop\n",
    "# this loop make a video to image folder with suffix \"_annotated\"\n",
    "with mytools.open_video_stream(input_filename) as stream, \\\n",
    "     mytools.open_video_writer(str(ann_path), stream.get(cv2.CAP_PROP_FRAME_WIDTH), stream.get(cv2.CAP_PROP_FRAME_HEIGHT)) as writer:\n",
    "\n",
    "    fps = 15 # you can specify input video FPS if you want\n",
    "    tracker = BYTETracker(\n",
    "        args=dict_dot_notation({\n",
    "            'track_thresh': 0.3,\n",
    "            'track_buffer': fps * 2,\n",
    "            'match_thresh': 0.8,\n",
    "            'mot20': False,\n",
    "        }),\n",
    "        frame_rate=fps\n",
    "    )\n",
    "    timeout_count_dict = {}\n",
    "    is_counted_dict = {}\n",
    "    trail_dict = {}\n",
    "    timeout_count_initial = fps\n",
    "    \n",
    "    progress = mytools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "    for batch_result in model.predict_batch(mytools.video_source(stream, report_error=False)):\n",
    "        # object detection\n",
    "        results = batch_result.results\n",
    "        bboxes = np.zeros((len(results), 5))\n",
    "        image = batch_result.image\n",
    "\n",
    "        # byte track\n",
    "        for index, result in enumerate(results):\n",
    "            bbox = np.array(result.get('bbox', [0, 0, 0, 0]))\n",
    "            score = result.get('score', 0)\n",
    "            bbox_and_score = np.append(bbox, score)\n",
    "            bboxes[index] = bbox_and_score\n",
    "\n",
    "        online_targets = tracker.update(bboxes, (1, 1), (1, 1))\n",
    "        online_target_set = set([])\n",
    "\n",
    "        # tracking start or continue\n",
    "        for target in online_targets:\n",
    "            tid = str(target.track_id)\n",
    "            online_target_set.add(str(tid))\n",
    "\n",
    "            box = tuple(map(int, target.tlbr)) # x1 y1 x2 y2\n",
    "            center = tuple(map(int, target.tlwh_to_xyah(target.tlwh)[:2]))\n",
    "            if trail_dict.get(tid, None) is None:\n",
    "                trail_dict[tid] = []\n",
    "            if is_counted_dict.get(tid, None) is None:\n",
    "                is_counted_dict[tid] = False\n",
    "            if not is_counted_dict[tid] and len(trail_dict[tid]) > 1:\n",
    "                trail_start = trail_dict[tid][0]\n",
    "                trail_end = center\n",
    "                is_cross = intersect(line_start, line_end, trail_start, trail_end)\n",
    "                if is_cross:\n",
    "                    if trail_start[0] > trail_end[0]:\n",
    "                        left += 1\n",
    "                    if trail_start[0] < trail_end[0]:\n",
    "                        right += 1\n",
    "                    if trail_start[1] < trail_end[1]:\n",
    "                        top += 1\n",
    "                    if trail_start[1] > trail_end[1]:\n",
    "                        bottom += 1\n",
    "                    is_counted_dict[tid] = True\n",
    "            trail_dict[tid].append(center)\n",
    "            timeout_count_dict[tid] = timeout_count_initial\n",
    "            if len(trail_dict[tid]) > 1:\n",
    "                cv2.polylines(image, [np.array(trail_dict[tid])], False, (255, 255, 0))\n",
    "            cv2.rectangle(image, box[0:2], box[2:4], color=(0, 255, 0), thickness=1)\n",
    "            cv2.drawMarker(image, center, (255, 255, 0), markerType=cv2.MARKER_CROSS)\n",
    "            cv2.putText(image, tid, box[0:2], cv2.FONT_HERSHEY_PLAIN, 1, color=(0, 255, 0), thickness=1)\n",
    "\n",
    "        # tracking terminate\n",
    "        for tid in set(timeout_count_dict.keys()) - online_target_set:\n",
    "            timeout_count_dict[tid] -= 1\n",
    "            if timeout_count_dict[tid] == 0:\n",
    "                del timeout_count_dict[tid], is_counted_dict[tid], trail_dict[tid]\n",
    "\n",
    "        text = 't:{} b:{} l:{} r:{}'.format(top, bottom, left, right)\n",
    "        cv2.putText(image, text, (10, image.shape[0]), cv2.FONT_HERSHEY_PLAIN, 1, color=(0, 255, 0), thickness=1)\n",
    "        cv2.line(image, line_start, line_end, (0, 255, 0))\n",
    "        \n",
    "        writer.write(image)\n",
    "        progress.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "50de14cfcf8437409e83adf65890e3e47263b30fd21ab1f0117168323be0df4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Parallel AI Inference on image dataset retrieved from a cloud\n",
    "This notebook is an example how to use DeGirum PySDK to do multi-threaded AI inference of an image dataset.\n",
    "Image dataset is retrieved from the cloud using `fiftyone` API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specify dataset parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the name of desired dataset to retrieve;\n",
    "# see https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html\n",
    "foz_dataset_name = \"coco-2017\"\n",
    "\n",
    "# specify list of class labels to retrieve; None for all classes\n",
    "foz_classes = None # [\"car\", \"cup\", \"person\"]\n",
    "\n",
    "# specify which splits to download (\"train\", \"validation\", \"test\")\n",
    "foz_splits = \"validation\"\n",
    "\n",
    "# specify # of samples in dataset to retrieve\n",
    "samples_num = 1000\n",
    "\n",
    "# specify the model to be used for inference\n",
    "model_name = \"yolo_v5s_coco--576x576_quant_n2x_orca_1\"\n",
    "\n",
    "# specify number of parallel threads to run inference\n",
    "nthreads = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg # import DeGirum PySDK\n",
    "import mytools\n",
    "import threading, queue, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f90e17",
   "metadata": {},
   "source": [
    "### Specify inference option here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc28d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please uncomment and edit one of the following inference options to specify your system configuration case according to\n",
    "# https://cs.degirum.com/doc/0.5.0/degirum.html#system-configuration-for-specific-use-cases\n",
    "\n",
    "# 1. DeGirum Cloud Zoo inference:\n",
    "#zoo = dg.connect_model_zoo(\"dgcps://cs.degirum.com\", token=mytools.token_get())\n",
    "\n",
    "# 2. AIServer inference via IP address using models from DeGirum Cloud model zoo\n",
    "#zoo = dg.connect_model_zoo((\"192.168.0.7\", \"https://cs.degirum.com/degirum_com/public\"), token=mytools.token_get())\n",
    "\n",
    "# 3. AIServer inference via IP address using local model zoo\n",
    "#zoo = dg.connect_model_zoo(\"192.168.0.1\")\n",
    "\n",
    "# 4. ORCA board installed locally using models from DeGirum Cloud Model Zoo\n",
    "#zoo = dg.connect_model_zoo(\"https://cs.degirum.com/degirum_com/public\", token=mytools.token_get())\n",
    "\n",
    "# 5. Local inference with locally deployed model\n",
    "#zoo = dg.connect_model_zoo(\"full/path/to/model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e512335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = mytools.import_fiftyone() # import 'fiftyone' package for dataset management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "dataset = fo.zoo.load_zoo_dataset(\n",
    "    foz_dataset_name,\n",
    "    dataset_dir=\"./my-dataset\", \n",
    "    classes=foz_classes,\n",
    "    split=foz_splits,\n",
    "    max_samples=samples_num,\n",
    "    shuffle=True,\n",
    "    drop_existing_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define worker thread function:\n",
    "# it takes frames from `inqueue`, performs ML inference, and puts results into `outqueue`\n",
    "def worker(inqueue, outqueue):\n",
    "        \n",
    "    # load model\n",
    "    model = zoo.load_model(model_name)\n",
    "    model.output_confidence_threshold = 0.1\n",
    "    myframes = []\n",
    "\n",
    "    # define frame source generator function:\n",
    "    # it gets frames from input queue and yields frame filenames one at a time\n",
    "    def source():\n",
    "        while True:\n",
    "            frame = inqueue.get()\n",
    "            if frame is None:\n",
    "                # poison pill received: return poison pill back to queue to stop other threads\n",
    "                inqueue.put(None)\n",
    "                break\n",
    "            myframes.append(frame)\n",
    "            yield frame.filepath\n",
    "\n",
    "    # do batch prediction on frame source\n",
    "    for res in model.predict_batch(source()):\n",
    "        # put a tuple of inference result and input frame into output queue\n",
    "        outqueue.put((res, myframes.pop(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338279d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create inference task and result queues\n",
    "task_queue = queue.Queue()\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# fill task queue with data from dataset\n",
    "nframes = dataset.count()\n",
    "print(f\"Queueing {nframes} frames...\")\n",
    "for frame in dataset:\n",
    "    task_queue.put(frame)\n",
    "\n",
    "task_queue.put(None) # poison pill is last\n",
    "\n",
    "# run ML inference in threads\n",
    "print(f\"Running inference...\")\n",
    "tmr = mytools.Timer()\n",
    "threads = []\n",
    "for n in range(nthreads): # start worker threads\n",
    "    t = threading.Thread(target=worker, args=(task_queue, result_queue))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads: # wait for completion\n",
    "    t.join()\n",
    "\n",
    "elapsed_s = tmr()\n",
    "print(f\"Inference done: {nframes} frames in {elapsed_s:.1f} s = {nframes/elapsed_s:.1f} FPS\")\n",
    "\n",
    "# process result queue: add inference results to dataset\n",
    "print(f\"Processing results...\")\n",
    "for n in range(nframes):\n",
    "    res, frame = result_queue.get()\n",
    "    w, h = res.image.size\n",
    "    detections = []\n",
    "    for box in res.results:\n",
    "        # Convert to [top-left-x, top-left-y, width, height]\n",
    "        # in relative coordinates in [0, 1] x [0, 1]\n",
    "        x1, y1, x2, y2 = box[\"bbox\"]\n",
    "        rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
    "        detections.append(fo.Detection(label=box[\"label\"], bounding_box=rel_box, confidence=box[\"score\"]))\n",
    "    frame[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    frame.save()\n",
    "\n",
    "print(f\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation on predictions\n",
    "eval_result = dataset.evaluate_detections(\"predictions\", compute_mAP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some evaluation results\n",
    "print( f\"mAP = {eval_result.mAP():.2f}\\n\")\n",
    "eval_result.print_report(classes=foz_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee99560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Tiled object detection from a video file with optional motion detection\n",
    "This notebook is an example how to use DeGirum PySDK to do tiled object detection of a video stream from a video file.\n",
    "Each video frame is divided by tiles with some overlap, each tile of the AI model input size (to avoid resizing).\n",
    "Object detection is performed for each tile, then results from different tiles are combined.\n",
    "The annotated video is saved into new file with `_tiled_annotated` suffix.\n",
    "If motion detection mode is turned on, then areas with motion are detected for each frame, and only tiles, where\n",
    "motion is detected, are processed.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. [DeGirum Cloud Platform](https://cs.degirum.com),\n",
    "1. DeGirum-hosted AI server node shared via Peer-to-Peer VPN,\n",
    "1. AI server node hosted by you in your local network,\n",
    "1. AI server running on your local machine,\n",
    "1. DeGirum ORCA accelerator directly installed on your local machine.\n",
    "\n",
    "\n",
    "To try different options, you just need to change the `inference_option` in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specify where do you want to run your inferences and video file name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_option = 3  # <<< change it according to your needs selecting from the list in the header comment\n",
    "input_filename = \"./images/TrafficHD.mp4\" # video file to process\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca_1\" # model to use\n",
    "min_overlap_precent = [20,20] # minimum tile overlap (in percent of tile dimensions)\n",
    "classes = [\"car\"] # list of classes to show\n",
    "do_motion_detection = True # enable motion detection: do inference only in tiles, where motion is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4549b06-567f-4cdb-8b82-bb4ef5858f2f",
   "metadata": {},
   "source": [
    "### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg # import DeGirum PySDK\n",
    "import mytools, cv2, math, threading, queue, numpy as np\n",
    "from pathlib import Path\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to model zoo according to selected inference option\n",
    "zoo = mytools.connect_model_zoo(inference_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load object detection model\n",
    "model = zoo.load_model(model_name)\n",
    "\n",
    "# set model parameters\n",
    "model.image_backend = 'opencv' # select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "model.input_numpy_colorspace = 'BGR'\n",
    "model.overlay_show_probabilities = False\n",
    "model.overlay_show_labels = False\n",
    "model.overlay_line_width = 1\n",
    "model.overlay_alpha = 1\n",
    "model._model_parameters.InputImgFmt = ['JPEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c581e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect areas with motion on given image in respect to base image.\n",
    "# Returns a tuple of motion image and updated base image.\n",
    "# Motion image is black image with white pixels where motion is detected.\n",
    "def detectMotion(base_img, img):\n",
    "\n",
    "    cur_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cur_img = cv2.GaussianBlur(src=cur_img, ksize=(5,5), sigmaX=0)\n",
    "    \n",
    "    if base_img is None:\n",
    "        base_img = cur_img\n",
    "        return None, base_img\n",
    "        \n",
    "    diff = cv2.absdiff(base_img, cur_img)    \n",
    "    base_img = cur_img\n",
    "    \n",
    "    _, thresh = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.dilate(thresh, None)\n",
    "    \n",
    "    return thresh, base_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3ecae-3162-4e6d-9157-6010a6db4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source of tile frames to be used in batch predict\n",
    "def source(stream, model, min_overlap_precent, progress):\n",
    "    \n",
    "    tile_w, tile_h = model.model_info.InputW[0], model.model_info.InputH[0]\n",
    "    image_w, image_h = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # function to calculate optimal overlap (0..1) and number of tiles\n",
    "    def calc_overlap(tile_dim, image_dim, min_overlap_precent):\n",
    "        tiles_less_one = math.ceil((image_dim - tile_dim) / (tile_dim * (1. - 0.01 * min_overlap_precent)))\n",
    "        return 1. - (image_dim - tile_dim) / (tiles_less_one * tile_dim), tiles_less_one + 1\n",
    "    \n",
    "    x_overlap, x_tiles = calc_overlap(tile_w, image_w, min_overlap_precent[0])\n",
    "    y_overlap, y_tiles = calc_overlap(tile_h, image_h, min_overlap_precent[1])\n",
    "    print(f\"Full frame: {image_w}x{image_h}, tile: {tile_w}x{tile_h}, overlap: {round(x_overlap*100)}x{round(y_overlap*100)}%, tiles: {x_tiles}x{y_tiles}={x_tiles*y_tiles}\")\n",
    "    \n",
    "    base_img = None # base imnage for motion detection\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        progress.step()\n",
    "        \n",
    "        # loop over tiles\n",
    "        first_tile = True\n",
    "        \n",
    "        if do_motion_detection:\n",
    "            motion_img, base_img = detectMotion(base_img, frame)\n",
    "            if motion_img is None:\n",
    "                continue\n",
    "        \n",
    "        for xi in range(x_tiles):\n",
    "            for yi in range(y_tiles):\n",
    "                x, y = math.floor(xi * tile_w * (1 - x_overlap)), math.floor(yi * tile_h * (1 - y_overlap))\n",
    "                \n",
    "                if do_motion_detection:\n",
    "                    if cv2.countNonZero(motion_img[y : y + tile_h, x : x + tile_w]) == 0:\n",
    "                        continue\n",
    "                \n",
    "                tile = frame[y : y + tile_h, x : x + tile_w]\n",
    "                info = { \"first_tile\": first_tile, \"frame\": frame, \"topleft\": (x, y), \"tilesize\": (tile_w, tile_h) }\n",
    "                first_tile = False\n",
    "                yield (tile, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253d6f5-b2b7-46b0-a6ff-0f5c5f3f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results of multiple tiles\n",
    "def combine(combined_result, new_result, iou_threshold=0.5):\n",
    "    \n",
    "    # filter classes\n",
    "    new_result._inference_results = [ res for res in new_result._inference_results if res[\"label\"] in classes ]\n",
    "    \n",
    "    # convert bbox coordinates to full image\n",
    "    topleft = new_result.info[\"topleft\"]\n",
    "    for r in new_result._inference_results:\n",
    "        r[\"bbox\"] = list(np.array(r[\"bbox\"]) + (topleft + topleft))\n",
    "    \n",
    "    if not combined_result:\n",
    "        # first tile result: just store\n",
    "        combined_result = new_result\n",
    "        combined_result._input_image = new_result.info[\"frame\"]\n",
    "    else:\n",
    "        # consecutive tile result: merge bboxes\n",
    "        for new_res in new_result._inference_results:\n",
    "            for res in combined_result._inference_results:\n",
    "                bboxes = np.array([new_res[\"bbox\"], res[\"bbox\"]])\n",
    "                areas = mytools.area(bboxes)\n",
    "                intersection = mytools.intersection(bboxes[0], bboxes[1])\n",
    "                if intersection / min(areas) >= iou_threshold:                   \n",
    "                    # take biggest box\n",
    "                    if areas[0] > areas[1]:\n",
    "                        res[\"bbox\"] = new_res[\"bbox\"]\n",
    "                    break\n",
    "            else: # this clause is executed when `for` loop has no breaks\n",
    "                # this box is genuine: just add it as is\n",
    "                combined_result._inference_results.append(new_res)\n",
    "    \n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906309-0ea3-458f-a1c4-282b2de56a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path = Path(input_filename)\n",
    "ann_path = orig_path.with_name(orig_path.stem + \"_tiled_annotated\" + orig_path.suffix)\n",
    "abort = False\n",
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "with mytools.Display(\"Tiled Detectoon\", not do_motion_detection) as display, \\\n",
    "     mytools.open_video_stream(input_filename) as stream, \\\n",
    "     mytools.open_video_writer(str(ann_path), stream.get(cv2.CAP_PROP_FRAME_WIDTH), stream.get(cv2.CAP_PROP_FRAME_HEIGHT)) as writer:     \n",
    "         \n",
    "    # do image processing in separate thread to improve performance\n",
    "    result_queue = queue.Queue()\n",
    "    def worker():\n",
    "        global abort\n",
    "        try:\n",
    "            while True:\n",
    "                result = result_queue.get()\n",
    "                if result is None:\n",
    "                    break;\n",
    "                img = result.image_overlay\n",
    "                writer.write(img)\n",
    "                \n",
    "                if do_motion_detection:\n",
    "                    mytools.Display.put_text(img, \n",
    "                        f\"Motion tiles: {result.info['tiles_cnt']:2d}\", (0, 0), (0, 0, 0), (255, 255, 255))\n",
    "                display.show(img)\n",
    "        except KeyboardInterrupt:\n",
    "            abort = True\n",
    "                \n",
    "    threading.Thread(target=worker).start()\n",
    "    \n",
    "    progress = mytools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "    combined_result = None\n",
    "    tiles_cnt = 0\n",
    "    \n",
    "    # inference loop\n",
    "    for res in model.predict_batch(source(stream, model, min_overlap_precent, progress)):\n",
    "        if res.info[\"first_tile\"] and combined_result: # new frame started\n",
    "            combined_result.info[\"tiles_cnt\"] = tiles_cnt\n",
    "            result_queue.put(combined_result)\n",
    "            combined_result = None\n",
    "            tiles_cnt = 0\n",
    "\n",
    "        combined_result = combine(combined_result, res)\n",
    "        tiles_cnt += 1\n",
    "        if abort:\n",
    "            break\n",
    "        \n",
    "    result_queue.put(None) # to stop worker thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211246e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display result\n",
    "IPython.display.Video(ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original video\n",
    "IPython.display.Video(orig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1619c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

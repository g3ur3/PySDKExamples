{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438aa03a",
   "metadata": {},
   "source": [
    "## This notebook is an example of how to pipeline two models. \n",
    "A video stream from a local camera is processed by the person detection model. The person detection results are then processed by the pose detection model, one person bbox at a time.\n",
    "Combined result is then displayed.\n",
    "OpenCV is required to run this sample.\n",
    "\n",
    "This script works with \n",
    "\n",
    "0. DeGirum Cloud Server, \n",
    "1. AI server equipped with DeGirum ORCA accelerator shared via Peer-to-Peer VPN, \n",
    "2. AI server equipped with DeGirum ORCA accelerator running in local network and \n",
    "3. AI server equipped with DeGirum ORCA accelerator running on the same machine as this code. \n",
    "\n",
    "To try different options, you just need to change the __model_zoo_option_index__ in the code below. \n",
    "\n",
    "The script needs a web camera connected to the machine running this code. The __camera_index__ also needs to be specified in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0f95d-7fd0-43b1-b94a-4835357d10a5",
   "metadata": {},
   "source": [
    "### Specify your model zoo option and camera index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37dddf-3f5f-40b0-8bd9-409ae37bc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo_option_list = ['DEGIRUM_CLOUD_SERVER', 'P2P_VPN_SERVER', 'LOCAL_NETWORK_SERVER', 'LOCALHOST']\n",
    "model_zoo_option_index=1\n",
    "camera_index=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dc838-9d2b-4806-b48c-67912b4fdebe",
   "metadata": {},
   "source": [
    "### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg # import DeGirum PySDK\n",
    "import cv2 # OpenCV\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "envs_loaded=load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "def show(img, capt = \"<image>\"):\n",
    "    # show opencv image\n",
    "    cv2.imshow(capt, img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('x') or key == ord('q'):\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "def crop(img, bbox):\n",
    "    # crop opencv image to given bbox\n",
    "    return img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5821a1-e08d-49d9-a5ba-de0bbb48bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo_option=model_zoo_option_list[model_zoo_option_index]\n",
    "if model_zoo_option=='DEGIRUM_CLOUD_SERVER':\n",
    "    zoo = dg.connect_model_zoo(os.getenv('DEGIRUM_CLOUD_SERVER_ADDRESS'), token=os.getenv('DEGIRUM_CLOUD_TOKEN'))\n",
    "elif model_zoo_option=='P2P_VPN_SERVER':\n",
    "    zoo=dg.connect_model_zoo(os.getenv('P2P_VPN_SERVER_ADDRESS'))\n",
    "elif model_zoo_option=='LOCAL_NETWORK_SERVER':\n",
    "    zoo=dg.connect_model_zoo(os.getenv('LOCAL_NETWORK_SERVER_ADDRESS'))\n",
    "elif model_zoo_option=='LOCALHOST':\n",
    "    zoo=dg.connect_model_zoo(os.getenv('LOCAL_HOST_ADDRESS'))\n",
    "else:\n",
    "    print('Model zoo option not supported \\n')                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33012fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "people_det_model = zoo.load_model(\"yolo_v5s_person_det--512x512_quant_n2x_orca_1\")\n",
    "pose_model = zoo.load_model(\"mobilenet_v1_posenet_coco_keypoints--353x481_quant_n2x_orca_1\")\n",
    "\n",
    "# adjust pose model properties\n",
    "pose_model.output_pose_threshold = 0.2 # lower threshold\n",
    "pose_model.overlay_line_width = 1\n",
    "pose_model.overlay_alpha = 1\n",
    "pose_model.overlay_show_probabilities = False\n",
    "pose_model.overlay_show_labels = False\n",
    "pose_model.image_backend = 'opencv' \n",
    "pose_model.input_numpy_colorspace='BGR'\n",
    "pose_model._model_parameters.InputImgFmt=['JPEG']\n",
    "\n",
    "# adjust people model properties\n",
    "people_det_model.image_backend = 'opencv'\n",
    "people_det_model._model_parameters.InputImgFmt=['JPEG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320e1c0-4c32-413b-bf47-78c699619a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open video stream from local camera \n",
    "stream = cv2.VideoCapture(camera_index)\n",
    "if (stream.isOpened()== False):\n",
    "    print(\"Error opening video stream\")\n",
    "else:\n",
    "    print(\"Succesful in opening video stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define iterator function, which returns frames from camera \n",
    "def source():\n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        yield frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93242ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "try:\n",
    "    # run person detection model on a camera stream\n",
    "    for people in people_det_model.predict_batch(source()):\n",
    "        # prepare list of bboxes of detected person\n",
    "        # if people is not None:\n",
    "        person_boxes = [person['bbox'] for person in people.results]\n",
    "        if len(person_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        # prepare list of images cropped around each detected person\n",
    "        person_crops = [ crop(people.image, box) for box in person_boxes ]\n",
    "\n",
    "        # for each detected person detect the pose\n",
    "        all_poses = None # accumulated result\n",
    "        for poses, box in zip(pose_model.predict_batch(person_crops), person_boxes):\n",
    "\n",
    "            for r in poses.results: # convert pose coordinates to back to original image\n",
    "                for p in r['landmarks']:\n",
    "                    p['landmark'][0] += box[0]\n",
    "                    p['landmark'][1] += box[1]\n",
    "\n",
    "            if all_poses is None: # accumulate all detected poses\n",
    "                all_poses = poses\n",
    "                all_poses._input_image = people.image_overlay\n",
    "            else:\n",
    "                all_poses._inference_results += poses.results\n",
    "\n",
    "        show(all_poses.image_overlay, \"Poses\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass # ignore KeyboardInterrupt errors\n",
    "finally:\n",
    "    cv2.destroyAllWindows() # close OpenCV windows\n",
    "stream.release() # release camera stream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Sliced object detection from a video file with optional motion detection\n",
    "This notebook is an example of how to use DeGirum PySDK to do sliced object detection of a video stream from a video file.\n",
    "Each video frame is divided by slices/tiles with some overlap, each tile of the AI model input size (to avoid resizing).\n",
    "Object detection is performed for each tile, then results from different tiles are combined.\n",
    "The annotated video is saved into new file with `_tiled_annotated` suffix.\n",
    "If motion detection mode is turned on, then areas with motion are detected for each frame, and only tiles, where\n",
    "motion is detected, are processed.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option.\n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b018f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify video file name, model name, and other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# video_source: video source for inference\n",
    "#     camera index for local camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "# model_name: name of the model for running AI inference\n",
    "# min_overlap_percent: minimum tile overlap (in percent of tile dimensions)\n",
    "# classes: list of classes to show\n",
    "# do_motion_detection: Boolean to specify if motion detection is enabled\n",
    "# ann_path: path to save annotated video\n",
    "hw_location = \"@cloud\"\n",
    "video_source = (\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/TrafficHD.mp4\"\n",
    ")\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca1_1\"\n",
    "min_overlap_percent = [20, 20]\n",
    "classes = [\"car\"]\n",
    "do_motion_detection = True\n",
    "ann_path = \"temp/sliced_object_detection.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1b821-e18e-403b-8147-9f95fc6cfa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512335c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "import cv2, math, threading, queue, numpy as np\n",
    "\n",
    "# connect to AI inference engine\n",
    "zoo = dg.connect(\n",
    "    hw_location, degirum_tools.get_cloud_zoo_url(), degirum_tools.get_token()\n",
    ")\n",
    "\n",
    "# load object detection model\n",
    "model = zoo.load_model(\n",
    "    model_name,\n",
    "    overlay_show_labels=False,\n",
    "    overlay_show_probabilities=False,\n",
    "    overlay_line_width=1,\n",
    "    overlay_alpha=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c581e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect areas with motion on given image in respect to base image.\n",
    "# Returns a tuple of motion image and updated base image.\n",
    "# Motion image is black image with white pixels where motion is detected.\n",
    "def detectMotion(base_img, img):\n",
    "    cur_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cur_img = cv2.GaussianBlur(src=cur_img, ksize=(5, 5), sigmaX=0)\n",
    "\n",
    "    if base_img is None:\n",
    "        base_img = cur_img\n",
    "        return None, base_img\n",
    "\n",
    "    diff = cv2.absdiff(base_img, cur_img)\n",
    "    base_img = cur_img\n",
    "\n",
    "    _, thresh = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.dilate(thresh, None)\n",
    "\n",
    "    return thresh, base_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3ecae-3162-4e6d-9157-6010a6db4964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define source of tile frames to be used in batch predict\n",
    "def source(stream, model, min_overlap_precent, progress):\n",
    "    tile_w, tile_h = model.model_info.InputW[0], model.model_info.InputH[0]\n",
    "    image_w, image_h = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(\n",
    "        stream.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    )\n",
    "\n",
    "    # function to calculate optimal overlap (0..1) and number of tiles\n",
    "    def calc_overlap(tile_dim, image_dim, min_overlap_precent):\n",
    "        tiles_less_one = math.ceil(\n",
    "            (image_dim - tile_dim) / (tile_dim * (1.0 - 0.01 * min_overlap_precent))\n",
    "        )\n",
    "        return (\n",
    "            1.0 - (image_dim - tile_dim) / (tiles_less_one * tile_dim),\n",
    "            tiles_less_one + 1,\n",
    "        )\n",
    "\n",
    "    x_overlap, x_tiles = calc_overlap(tile_w, image_w, min_overlap_precent[0])\n",
    "    y_overlap, y_tiles = calc_overlap(tile_h, image_h, min_overlap_precent[1])\n",
    "    print(\n",
    "        f\"Full frame: {image_w}x{image_h}, tile: {tile_w}x{tile_h}, overlap: {round(x_overlap*100)}x{round(y_overlap*100)}%, tiles: {x_tiles}x{y_tiles}={x_tiles*y_tiles}\"\n",
    "    )\n",
    "\n",
    "    base_img = None  # base image for motion detection\n",
    "\n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        progress.step()\n",
    "\n",
    "        # loop over tiles\n",
    "        first_tile = True\n",
    "\n",
    "        if do_motion_detection:\n",
    "            motion_img, base_img = detectMotion(base_img, frame)\n",
    "            if motion_img is None:\n",
    "                continue\n",
    "\n",
    "        for xi in range(x_tiles):\n",
    "            for yi in range(y_tiles):\n",
    "                x, y = math.floor(xi * tile_w * (1 - x_overlap)), math.floor(\n",
    "                    yi * tile_h * (1 - y_overlap)\n",
    "                )\n",
    "\n",
    "                if do_motion_detection:\n",
    "                    if (\n",
    "                        cv2.countNonZero(motion_img[y : y + tile_h, x : x + tile_w])\n",
    "                        == 0\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                tile = frame[y : y + tile_h, x : x + tile_w]\n",
    "                info = {\n",
    "                    \"first_tile\": first_tile,\n",
    "                    \"frame\": frame,\n",
    "                    \"topleft\": (x, y),\n",
    "                    \"tilesize\": (tile_w, tile_h),\n",
    "                }\n",
    "                first_tile = False\n",
    "                yield (tile, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253d6f5-b2b7-46b0-a6ff-0f5c5f3f8dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine results of multiple tiles\n",
    "def combine(combined_result, new_result, iou_threshold=0.5):\n",
    "    # filter classes\n",
    "    new_result._inference_results = [\n",
    "        inference_result\n",
    "        for inference_result in new_result._inference_results\n",
    "        if inference_result.get(\"label\") in classes\n",
    "    ]\n",
    "\n",
    "    # convert bbox coordinates to full image\n",
    "    topleft = new_result.info[\"topleft\"]\n",
    "    for r in new_result._inference_results:\n",
    "        r[\"bbox\"] = list(np.array(r[\"bbox\"]) + (topleft + topleft))\n",
    "\n",
    "    if not combined_result:\n",
    "        # first tile result: just store\n",
    "        combined_result = new_result\n",
    "        combined_result._input_image = new_result.info[\"frame\"]\n",
    "    else:\n",
    "        # consecutive tile result: merge bboxes\n",
    "        for new_inference_result in new_result._inference_results:\n",
    "            for inference_result in combined_result._inference_results:\n",
    "                bboxes = np.array(\n",
    "                    [new_inference_result[\"bbox\"], inference_result[\"bbox\"]]\n",
    "                )\n",
    "                areas = degirum_tools.area(bboxes)\n",
    "                intersection = degirum_tools.intersection(bboxes[0], bboxes[1])\n",
    "                if intersection / min(areas) >= iou_threshold:\n",
    "                    # take biggest box\n",
    "                    if areas[0] > areas[1]:\n",
    "                        inference_result[\"bbox\"] = new_inference_result[\"bbox\"]\n",
    "                    break\n",
    "            else:  # this clause is executed when `for` loop has no breaks\n",
    "                # this box is genuine: just add it as is\n",
    "                combined_result._inference_results.append(new_inference_result)\n",
    "\n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906309-0ea3-458f-a1c4-282b2de56a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abort = False\n",
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "with degirum_tools.Display(\n",
    "    \"Tiled Detection\", not do_motion_detection\n",
    ") as display, degirum_tools.open_video_stream(\n",
    "    video_source\n",
    ") as stream, degirum_tools.open_video_writer(\n",
    "    str(ann_path),\n",
    "    stream.get(cv2.CAP_PROP_FRAME_WIDTH),\n",
    "    stream.get(cv2.CAP_PROP_FRAME_HEIGHT),\n",
    ") as writer:\n",
    "    # do image processing in separate thread to improve performance\n",
    "    result_queue = queue.Queue()\n",
    "\n",
    "    def worker():\n",
    "        global abort\n",
    "        try:\n",
    "            while True:\n",
    "                result = result_queue.get()\n",
    "                if result is None:\n",
    "                    break\n",
    "                img = result.image_overlay\n",
    "                writer.write(img)\n",
    "\n",
    "                if do_motion_detection:\n",
    "                    degirum_tools.put_text(\n",
    "                        img,\n",
    "                        f\"Motion tiles: {result.info['tiles_cnt']:2d}\",\n",
    "                        (0, 0),\n",
    "                        font_color=(0, 0, 0),\n",
    "                        bg_color=(255, 255, 255),\n",
    "                    )\n",
    "                display.show(img)\n",
    "        except KeyboardInterrupt:\n",
    "            abort = True\n",
    "\n",
    "    worker_thread = threading.Thread(target=worker)\n",
    "    worker_thread.start()\n",
    "\n",
    "    progress = degirum_tools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "    combined_result = None\n",
    "    tiles_cnt = 0\n",
    "\n",
    "    # inference loop\n",
    "    for inference_result in model.predict_batch(\n",
    "        source(stream, model, min_overlap_percent, progress)\n",
    "    ):\n",
    "        if inference_result.info[\"first_tile\"] and combined_result:  # new frame started\n",
    "            combined_result.info[\"tiles_cnt\"] = tiles_cnt\n",
    "            result_queue.put(combined_result)\n",
    "            combined_result = None\n",
    "            tiles_cnt = 0\n",
    "\n",
    "        combined_result = combine(combined_result, inference_result)\n",
    "        tiles_cnt += 1\n",
    "        if abort:\n",
    "            break\n",
    "\n",
    "    result_queue.put(None)  # to stop worker thread\n",
    "\n",
    "    worker_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211246e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display result\n",
    "degirum_tools.ipython_display(ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original video\n",
    "degirum_tools.ipython_display(video_source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Sliced object detection from a video file with optional motion detection\n",
    "This notebook is an example of how to use DeGirum PySDK to do sliced object detection of a video stream from a video file.\n",
    "Each video frame is divided by slices/tiles with some overlap, each tile of the AI model input size (to avoid resizing).\n",
    "Object detection is performed for each tile, then results from different tiles are combined.\n",
    "The annotated video is saved into new file with `_tiled_annotated` suffix.\n",
    "If motion detection mode is turned on, then areas with motion are detected for each frame, and only tiles, where\n",
    "motion is detected, are processed.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](../../env.ini) file, located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify video file name, model name, and other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_filename = \"../../images/TrafficHD.mp4\"  # video file to process\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca1_1\"  # model to use\n",
    "min_overlap_precent = [20, 20]  # minimum tile overlap (in percent of tile dimensions)\n",
    "classes = [\"car\"]  # list of classes to show\n",
    "do_motion_detection = True  # enable motion detection: do inference only in tiles, where motion is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533830-1888-4c56-8883-1d53bb81b1e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "degirum_tools.configure_colab()  # configure for Google Colab\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "hw_location = dg.CLOUD  # <-- on the Cloud Platform\n",
    "# hw_location = degirum_tools.get_ai_server_hostname() # <-- on AI Server deployed in your LAN\n",
    "# hw_location = dg.LOCAL # <-- on ORCA accelerator installed on this computer\n",
    "\n",
    "# connect to AI inference engine getting zoo URL and token from env.ini file\n",
    "zoo = dg.connect(\n",
    "    hw_location, degirum_tools.get_cloud_zoo_url(), degirum_tools.get_token()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1b821-e18e-403b-8147-9f95fc6cfa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512335c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2, math, threading, queue, numpy as np\n",
    "from pathlib import Path\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f3bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load object detection model\n",
    "model = zoo.load_model(model_name)\n",
    "\n",
    "# set model parameters\n",
    "model.overlay_show_probabilities = False\n",
    "model.overlay_show_labels = False\n",
    "model.overlay_line_width = 1\n",
    "model.overlay_alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c581e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect areas with motion on given image in respect to base image.\n",
    "# Returns a tuple of motion image and updated base image.\n",
    "# Motion image is black image with white pixels where motion is detected.\n",
    "def detectMotion(base_img, img):\n",
    "    cur_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cur_img = cv2.GaussianBlur(src=cur_img, ksize=(5, 5), sigmaX=0)\n",
    "\n",
    "    if base_img is None:\n",
    "        base_img = cur_img\n",
    "        return None, base_img\n",
    "\n",
    "    diff = cv2.absdiff(base_img, cur_img)\n",
    "    base_img = cur_img\n",
    "\n",
    "    _, thresh = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.dilate(thresh, None)\n",
    "\n",
    "    return thresh, base_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3ecae-3162-4e6d-9157-6010a6db4964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define source of tile frames to be used in batch predict\n",
    "def source(stream, model, min_overlap_precent, progress):\n",
    "    tile_w, tile_h = model.model_info.InputW[0], model.model_info.InputH[0]\n",
    "    image_w, image_h = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(\n",
    "        stream.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    )\n",
    "\n",
    "    # function to calculate optimal overlap (0..1) and number of tiles\n",
    "    def calc_overlap(tile_dim, image_dim, min_overlap_precent):\n",
    "        tiles_less_one = math.ceil(\n",
    "            (image_dim - tile_dim) / (tile_dim * (1.0 - 0.01 * min_overlap_precent))\n",
    "        )\n",
    "        return (\n",
    "            1.0 - (image_dim - tile_dim) / (tiles_less_one * tile_dim),\n",
    "            tiles_less_one + 1,\n",
    "        )\n",
    "\n",
    "    x_overlap, x_tiles = calc_overlap(tile_w, image_w, min_overlap_precent[0])\n",
    "    y_overlap, y_tiles = calc_overlap(tile_h, image_h, min_overlap_precent[1])\n",
    "    print(\n",
    "        f\"Full frame: {image_w}x{image_h}, tile: {tile_w}x{tile_h}, overlap: {round(x_overlap*100)}x{round(y_overlap*100)}%, tiles: {x_tiles}x{y_tiles}={x_tiles*y_tiles}\"\n",
    "    )\n",
    "\n",
    "    base_img = None  # base imnage for motion detection\n",
    "\n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        progress.step()\n",
    "\n",
    "        # loop over tiles\n",
    "        first_tile = True\n",
    "\n",
    "        if do_motion_detection:\n",
    "            motion_img, base_img = detectMotion(base_img, frame)\n",
    "            if motion_img is None:\n",
    "                continue\n",
    "\n",
    "        for xi in range(x_tiles):\n",
    "            for yi in range(y_tiles):\n",
    "                x, y = math.floor(xi * tile_w * (1 - x_overlap)), math.floor(\n",
    "                    yi * tile_h * (1 - y_overlap)\n",
    "                )\n",
    "\n",
    "                if do_motion_detection:\n",
    "                    if (\n",
    "                        cv2.countNonZero(motion_img[y : y + tile_h, x : x + tile_w])\n",
    "                        == 0\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                tile = frame[y : y + tile_h, x : x + tile_w]\n",
    "                info = {\n",
    "                    \"first_tile\": first_tile,\n",
    "                    \"frame\": frame,\n",
    "                    \"topleft\": (x, y),\n",
    "                    \"tilesize\": (tile_w, tile_h),\n",
    "                }\n",
    "                first_tile = False\n",
    "                yield (tile, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253d6f5-b2b7-46b0-a6ff-0f5c5f3f8dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine results of multiple tiles\n",
    "def combine(combined_result, new_result, iou_threshold=0.5):\n",
    "    # filter classes\n",
    "    new_result._inference_results = [\n",
    "        inference_result\n",
    "        for inference_result in new_result._inference_results\n",
    "        if inference_result.get(\"label\") in classes\n",
    "    ]\n",
    "\n",
    "    # convert bbox coordinates to full image\n",
    "    topleft = new_result.info[\"topleft\"]\n",
    "    for r in new_result._inference_results:\n",
    "        r[\"bbox\"] = list(np.array(r[\"bbox\"]) + (topleft + topleft))\n",
    "\n",
    "    if not combined_result:\n",
    "        # first tile result: just store\n",
    "        combined_result = new_result\n",
    "        combined_result._input_image = new_result.info[\"frame\"]\n",
    "    else:\n",
    "        # consecutive tile result: merge bboxes\n",
    "        for new_inference_result in new_result._inference_results:\n",
    "            for inference_result in combined_result._inference_results:\n",
    "                bboxes = np.array(\n",
    "                    [new_inference_result[\"bbox\"], inference_result[\"bbox\"]]\n",
    "                )\n",
    "                areas = degirum_tools.area(bboxes)\n",
    "                intersection = degirum_tools.intersection(bboxes[0], bboxes[1])\n",
    "                if intersection / min(areas) >= iou_threshold:\n",
    "                    # take biggest box\n",
    "                    if areas[0] > areas[1]:\n",
    "                        inference_result[\"bbox\"] = new_inference_result[\"bbox\"]\n",
    "                    break\n",
    "            else:  # this clause is executed when `for` loop has no breaks\n",
    "                # this box is genuine: just add it as is\n",
    "                combined_result._inference_results.append(new_inference_result)\n",
    "\n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906309-0ea3-458f-a1c4-282b2de56a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_path = Path(input_filename)\n",
    "ann_path = \"../../workarea/\" + orig_path.stem + \"_tiled_annotated\" + orig_path.suffix\n",
    "abort = False\n",
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "with degirum_tools.Display(\n",
    "    \"Tiled Detection\", not do_motion_detection\n",
    ") as display, degirum_tools.open_video_stream(\n",
    "    input_filename\n",
    ") as stream, degirum_tools.open_video_writer(\n",
    "    str(ann_path),\n",
    "    stream.get(cv2.CAP_PROP_FRAME_WIDTH),\n",
    "    stream.get(cv2.CAP_PROP_FRAME_HEIGHT),\n",
    ") as writer:\n",
    "    # do image processing in separate thread to improve performance\n",
    "    result_queue = queue.Queue()\n",
    "\n",
    "    def worker():\n",
    "        global abort\n",
    "        try:\n",
    "            while True:\n",
    "                result = result_queue.get()\n",
    "                if result is None:\n",
    "                    break\n",
    "                img = result.image_overlay\n",
    "                writer.write(img)\n",
    "\n",
    "                if do_motion_detection:\n",
    "                    degirum_tools.put_text(\n",
    "                        img,\n",
    "                        f\"Motion tiles: {result.info['tiles_cnt']:2d}\",\n",
    "                        (0, 0),\n",
    "                        font_color=(0, 0, 0),\n",
    "                        bg_color=(255, 255, 255),\n",
    "                    )\n",
    "                display.show(img)\n",
    "        except KeyboardInterrupt:\n",
    "            abort = True\n",
    "\n",
    "    worker_thread = threading.Thread(target=worker)\n",
    "    worker_thread.start()\n",
    "\n",
    "    progress = degirum_tools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "    combined_result = None\n",
    "    tiles_cnt = 0\n",
    "\n",
    "    # inference loop\n",
    "    for inference_result in model.predict_batch(\n",
    "        source(stream, model, min_overlap_precent, progress)\n",
    "    ):\n",
    "        if inference_result.info[\"first_tile\"] and combined_result:  # new frame started\n",
    "            combined_result.info[\"tiles_cnt\"] = tiles_cnt\n",
    "            result_queue.put(combined_result)\n",
    "            combined_result = None\n",
    "            tiles_cnt = 0\n",
    "\n",
    "        combined_result = combine(combined_result, inference_result)\n",
    "        tiles_cnt += 1\n",
    "        if abort:\n",
    "            break\n",
    "\n",
    "    result_queue.put(None)  # to stop worker thread\n",
    "\n",
    "    worker_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211246e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display result\n",
    "IPython.display.Video(ann_path, embed=degirum_tools._in_colab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original video\n",
    "IPython.display.Video(orig_path, embed=degirum_tools._in_colab())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dg_supermicro)",
   "language": "python",
   "name": "dg_supermicro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ecb4806ce01c16f7273aa67826524f8880d9fc434c49c35172b7bcd4b045cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f257328f",
   "metadata": {},
   "source": [
    "## Example script illustrating asynchronous parallel execution of sound classification on audio stream and object detection on video stream\n",
    "This notebook is an example how to use DeGirum PySDK to perform parallel inferences on two asynchronous data streams with different frame rates. To achieve maximum performance this example uses non-blocking batch prediction mode.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "**pyaudio package with portaudio is required to run this sample.**\n",
    "\n",
    "**Access to microphone is required to run this sample.**\n",
    "\n",
    "**Access to camera is required to run this sample.**\n",
    "\n",
    "The script needs either a web camera or local camera connected to the machine running this code. The camera index or URL needs to be specified either in the code below by assigning `camera_id` or in [env.ini](env.ini) file by defining `CAMERA_ID` variable and assigning `camera_id = None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9de969",
   "metadata": {},
   "source": [
    "#### Specify camera id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c22fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camera_id = None         # camera index or URL; 0 to use default local camera, None to take from env.ini file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9699a5-be1b-42ca-af2b-8233eb98d34f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54172f00-f82f-4122-b560-59e172598afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "cloud_token = mytools.get_token() # get cloud API access token from env.ini file\n",
    "cloud_zoo_url = mytools.get_cloud_zoo_url() # get cloud zoo URL from env.ini file\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "# 1. Inference on the DeGirum Cloud Platform\n",
    "zoo = dg.connect(dg.CLOUD, cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 2. Inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN\n",
    "# zoo = dg.connect(mytools.get_ai_server_hostname(), cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 3. Inference on DeGirum ORCA accelerator installed on your computer\n",
    "# zoo = dg.connect(dg.LOCAL, cloud_zoo_url, cloud_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15311e-8aed-466d-a11a-bed02b38be33",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a1753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mytools\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd775c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load YAMNET sound classification model for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "sound_model = zoo.load_model(\"mobilenet_v1_yamnet_sound_cls--96x64_quant_n2x_orca_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f1b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load MobileNetv2+SSD object detection model for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "detection_model = zoo.load_model(\"mobilenet_v2_ssd_coco--300x300_quant_n2x_orca_1\")\n",
    "\n",
    "# set model parameters\n",
    "detection_model.image_backend = 'opencv' # select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "detection_model.input_numpy_colorspace = 'BGR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da875f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set non-blocking mode for both models\n",
    "sound_model.non_blocking_batch_predict = True\n",
    "detection_model.non_blocking_batch_predict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db989e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_sampling_rate_hz = sound_model.model_info.InputSamplingRate[0]\n",
    "audio_buffer_size = sound_model.model_info.InputWaveformSize[0] // 2 # two read buffers in waveform for half-length overlapping\n",
    "\n",
    "with mytools.Display(\"Async Streams\") as display, \\\n",
    "    mytools.open_audio_stream(audio_sampling_rate_hz, audio_buffer_size) as audio_stream, \\\n",
    "    mytools.open_video_stream(camera_id) as video_stream:\n",
    "    \n",
    "    # create prediction result generators:\n",
    "    sound_predictor = sound_model.predict_batch(mytools.audio_overlapped_source(audio_stream, lambda: False, True))\n",
    "    detection_predictor = detection_model.predict_batch(mytools.video_source(video_stream))\n",
    "    \n",
    "    sound_label = \"\"\n",
    "    while True: # press 'x' or 'q' to abort\n",
    "        \n",
    "        # do asynchronous ML inferences for both models (each one can be None if not ready):\n",
    "        sound_result = next(sound_predictor)\n",
    "        detection_result = next(detection_predictor)\n",
    "\n",
    "        # process sound classification result (just remember the text)\n",
    "        if sound_result is not None:\n",
    "            sound_label = f\"{sound_result.results[0]['label']}: {sound_result.results[0]['score']}\"\n",
    "        \n",
    "        # process video detection result (just display the annotated frame)\n",
    "        if detection_result is not None:\n",
    "            img = detection_result.image_overlay\n",
    "            mytools.Display.put_text(img, sound_label, (1, img.shape[0] - 40), (0,0,0), (255,255,255))\n",
    "            display.show(img)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e24b44-9cee-4e4f-8979-b5741588568c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
